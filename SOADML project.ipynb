{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.ensai.fr/files/_media/images/l_ecole/Partenaires/Ecoles/ensae_logo_dev.png\" width=\"400\" height=\"80\" />\n",
    "<br>\n",
    "<div class=\"monika\" align=\"center\" style=\"font-size:200%\"> Stochastic Optimization and Automatic Differentiation for Machine Learning </div>\n",
    "<br>\n",
    "<div style=\"text-align: center\"> Selim Dekali & Mathis Linger </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n) {\n",
       "        a += \"    \";\n",
       "    }\n",
       "    return a;\n",
       "}\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    for (i = 0; i <= llast; i++) {\n",
       "        tags.push(\"h\" + i);\n",
       "    }\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null){\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\")\n",
       "        }\n",
       "\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += \"</ul>\\n\";\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2) + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<li><a href=\"#__HREF__\">__TITLE__</a></li>';\n",
       "    var send = \"\";\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 2, 4, sformat, send, keep_item);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jyquickhelper import add_notebook_menu\n",
    "add_notebook_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1] Sklearn benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.svm import LinearSVC\n",
    "import sklearn.metrics.pairwise as kernel\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=20)\n",
    "np.place(y, y==0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X, y)\n",
    "sklearn_coefs = clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider:\n",
    "- $x_1,..., x_n$ $\\in$ $\\mathbb{R}^d$\n",
    "- $y_1,...,y_n$ $\\in$ $(-1, 1)$\n",
    "- $\\lambda > 0$ a regularization parameter\n",
    "- $\\phi_1,...,\\phi_n$ a sequence of scalars convex functions\n",
    "- Since we are working on the SVM problem (with linear kernels and no bias term), we set $\\phi_i(a) = max(0,1-y_ia)$ (Hinge loss)\n",
    "\n",
    "For solving SVM, there are 2 approaches:\n",
    "- stochastic gradient descent (SGD), which aims to minimize the following primal problem:\n",
    "\\begin{equation}\n",
    "min_{w \\text{ } \\in \\text{ } R^d} \\big[ \\frac{1}{n} \\sum\\limits_{i=1}^n \\phi_i(w^Tx_i) + \\frac{\\lambda}{2}||w||^2\\big]\n",
    "\\end{equation}\n",
    "- dual coordinate ascent (DCA), which aims to maximize the following dual problem:\n",
    "\\begin{equation}\n",
    "max_{\\alpha \\text{ } \\in \\text{ } R^n} \\big[ \\frac{1}{n} \\sum\\limits_{i=1}^n -\\phi_i^*(-\\alpha_i) - \\frac{\\lambda}{2} \\big|\\big|\\frac{1}{\\lambda n}\\sum\\limits_{i=1}^n \\alpha_i x_i \\big|\\big|^2\\big]\n",
    "\\end{equation}\n",
    "here, for each $i$, $\\phi_i^*: \\mathbb{R} \\rightarrow \\mathbb{R}$ is the convex conjugate of $\\phi_i$, namely $\\phi_i^*(u) = max_z(zu - \\phi_i(z))$\n",
    "\n",
    "If we define, $w(\\alpha) = \\frac{1}{\\lambda n}\\sum\\limits_{i=1}^n \\alpha_i x_i$, then we have that $w(\\alpha^*) = w^*$ where $\\alpha^*$ is an optimal solution of the dual problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2] Dual coordinate ascent for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___SDCA___</center>\n",
    "***\n",
    "___Let___ $w^{(0)} = w(\\alpha^{(0)})$<br>\n",
    "___Iterate:___ for $t=1,2,...,T:$\n",
    "> Randomly pick $i$<br>\n",
    "Find $\\Delta\\alpha_i$ to maximize $-\\phi_i^*(-(\\alpha_i^{(t-1)}+\\Delta\\alpha_i)) - \\frac{\\lambda n}{2} ||w^{(t-1)}+(\\lambda n)^{-1} \\Delta\\alpha_ix_i||^2$<br>\n",
    "$\\alpha^{(t)} \\leftarrow \\alpha^{(t-1)}+\\Delta\\alpha_ie_i$<br>\n",
    "$w^{(t)} \\leftarrow w^{(t-1)}+(\\lambda n)^{-1}\\Delta\\alpha_ix_i$<br>\n",
    " \n",
    "___Output (Averaging option):___<br>\n",
    "> Let $\\bar{\\alpha} = \\frac{1}{T-T_0}\\sum\\limits_{i=T_0+1}^T \\alpha^{(t-1)}$<br>\n",
    "Let $\\bar{w} = w(\\bar{\\alpha}) = \\frac{1}{T-T_0}\\sum\\limits_{i=T_0+1}^T w^{(t-1)}$<br>\n",
    "return $\\bar{w}$<br>\n",
    "\n",
    "___Output (Random option):___<br>\n",
    "> Let $\\bar{\\alpha} = \\alpha^{(t)}$ and $\\bar{w} = w^{(t)}$ for some random $t \\in T_0+1,...T$<br>\n",
    "return $\\bar{w}$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Close formula to maximize $-\\phi_i^*(-(\\alpha_i^{(t-1)}+\\Delta\\alpha_i)) - \\frac{\\lambda n}{2} ||w^{(t-1)}+(\\lambda n)^{-1} \\Delta\\alpha_ix_i||^2$:___\n",
    "\n",
    "We want to $max_{\\alpha \\text{ } \\in \\text{ } R^n} D(\\alpha)$ with $D(\\alpha) = \\frac{1}{n} \\sum\\limits_{i=1}^n -\\phi_i^*(-\\alpha_i) - \\frac{\\lambda}{2} \\big|\\big|\\frac{1}{\\lambda n}\\sum\\limits_{i=1}^n \\alpha_i x_i \\big|\\big|^2 = - \\frac{1}{2\\lambda n^2} \\alpha^T X^T X \\alpha + \\frac{1}{n} \\sum\\limits_{i=1}^n -\\phi_i^*(-\\alpha_i)$\n",
    "\n",
    "At each iteration, we have to maximize the updated dual objective defined as:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "D(\\alpha_t+\\Delta\\alpha_i e_i) = &\\frac{-1}{2\\lambda n^2} (\\alpha_t+\\Delta\\alpha_i e_i)^T X^T X (\\alpha_t+\\Delta\\alpha_i e_i) - \\frac{1}{n}\\sum\\limits_{i=1}^n \\phi_i^*(-\\alpha_t-\\Delta\\alpha_i e_i)\\\\\n",
    "=& \\frac{-1}{2\\lambda n^2} \\alpha_t^T X^T X \\alpha_t - \\frac{1}{\\lambda n^2} \\alpha_t^T X^T X \\Delta\\alpha_ie_i - \\frac{1}{2\\lambda n^2} (\\Delta\\alpha_i e_i)^T X^T X (\\Delta\\alpha_i e_i) - \\frac{1}{n}\\sum\\limits_{i=1}^n \\phi_i^*(-\\alpha_t-\\Delta\\alpha_i e_i)\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "By setting:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "Constant =& \\frac{-1}{2\\lambda n^2} \\alpha_t^T X^T X \\alpha_t - \\frac{1}{n}\\sum\\limits_{\\substack{i=1 \\\\ i\\neq j}}^n \\phi_j^*(-\\alpha_t-\\Delta\\alpha_j e_j)\\\\\n",
    "A =& \\frac{1}{\\lambda n} x_i^T x_i = \\frac{1}{\\lambda n}||x_i||^2\\\\\n",
    "B=& x_i^T \\frac{X \\alpha_t}{\\lambda n} = x_i^T w_t\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "we get:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "D(\\alpha_t+\\Delta\\alpha_i e_i) \\propto \\frac{-A}{2} (\\Delta\\alpha_i)^2 - B \\Delta\\alpha_i - \\phi^*_i(-\\alpha_i\\Delta\\alpha_i)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We recall that in the case of SVM, we use the Hinge loss, i.e.:\n",
    "\\begin{equation}\n",
    "\\phi_i(a) = max(0,1-y_ia)\n",
    "\\end{equation}\n",
    "and its conjugate is given by:\n",
    "\\begin{equation}\n",
    "\\phi^*_i(u) =\n",
    "    \\begin{cases}\n",
    "      y_i u & \\text{if} -1\\leq y_iu\\leq 0\\\\\n",
    "      +\\infty & \\text{otherwise}\n",
    "    \\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "Then, considering that $-1\\leq y_iu\\leq 0$, we can maximize $D(\\alpha_t+\\Delta\\alpha_i e_i)$ by $\\Delta\\alpha_i$ by:\n",
    "\\begin{equation}\n",
    "\\tilde{\\Delta\\alpha_i} = \\frac{y_i -B}{A}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, if we incorporate the constraint $-1\\leq -y_i(\\alpha_i + \\Delta\\alpha_i)\\leq 0 \\leftrightarrow 0\\leq y_i(\\alpha_i + \\Delta\\alpha_i)\\leq 1$, we obtain the update:\n",
    "\\begin{equation}\n",
    "\\Delta\\alpha_i = \\frac{1}{y_i}max(0, min(1, y_i(\\alpha_i + \\tilde{\\Delta\\alpha_i}))) - \\alpha_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dual_primal(alpha, X, lambda_):\n",
    "    return X.T.dot(alpha)/(lambda_*X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, we set $T_0 = \\frac{T}{2}$ for the averaging option as suggested in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SDCA(X, y, lambda_ = 1, nb_iteration = 100000, output = 'averaging'):\n",
    "    n = X.shape[0]\n",
    "    alpha = [np.zeros(n)]\n",
    "    w = [dual_primal(alpha[0], X, lambda_)]\n",
    "    for t in tqdm(range(nb_iteration)):\n",
    "        i = np.random.randint(0, n)\n",
    "        A = (1/(lambda_*n))*np.linalg.norm(X[i], 2)**2\n",
    "        B = X[i].dot(w[t])\n",
    "        delta_alpha = (y[i]-B)/A\n",
    "        delta_alpha_constrained = (1/y[i])*max(0, min(1, y[i]*(delta_alpha+alpha[t][i]))) - alpha[t][i]\n",
    "        e_i = np.zeros(n)\n",
    "        e_i[i] = 1.\n",
    "        alpha.append(alpha[t]+delta_alpha_constrained*e_i)\n",
    "        w.append(dual_primal(alpha[t+1], X, lambda_))\n",
    "    if output == 'averaging':\n",
    "        return np.mean(w[int(nb_iteration/2):], axis = 0)\n",
    "    else:\n",
    "        return w[np.random.randint(int(nb_iteration/2), nb_iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:08<00:00, 12278.98it/s]\n"
     ]
    }
   ],
   "source": [
    "sdca_coefs = SDCA(X, y, 1, 100000, output = 'averaging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0026084592611657765"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(sdca_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sdca = sdca_coefs.dot(X.T)\n",
    "pred_sdca = (pred_sdca > 0).astype(int)\n",
    "np.place(pred_sdca, pred_sdca==0, -1)\n",
    "sum(abs(pred_sdca - y)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found a paper proposing a mini batch version of SDCA. We can implement it in order to compare with the stochastic version. The paper is available at https://pdfs.semanticscholar.org/b0b5/13b601e28db45a02ed4b19801af0cb29e462.pdf#page3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original optimization problem:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min_{w,b} \\frac{1}{2} w.w \\\\\n",
    "& \\text{subject to : } (w.x_j) y_j \\geq 1 \\forall j\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We will use the Lagragian to solve the problem:\n",
    "$$\n",
    "L(w,\\alpha) = \\frac{1}{2}w.w - \\sum_{j} \\alpha_j \\big[(w.x_j+b)y_j-1 \\big]\n",
    "$$\n",
    "Our goal now is to solve: $$\\min_{w,b} \\max_{\\alpha} L(w, \\alpha) $$\n",
    "(primal) :  $$ \\min_{w,b} \\max_{\\alpha} \\frac{1}{2}w.w - \\sum_{j} \\alpha_j \\big[(w.x_j+b)y_j-1 \\big]$$\n",
    "\n",
    "(dual) : $$ \\max_{\\alpha} \\min_{w,b} \\frac{1}{2}w.w - \\sum_{j} \\alpha_j \\big[(w.x_j+b)y_j-1 \\big]$$\n",
    "\n",
    "Can solve for optimal w, b as function of α:\n",
    "$$\\frac{\\partial L}{\\partial w} = w - \\sum_{j} \\alpha_j x_j y_j \\rightarrow w = \\sum_{j} \\alpha_j x_j y_j$$\n",
    "$$\\frac{\\partial L}{\\partial b} = - \\sum_{j} \\alpha_j y_j \\rightarrow  \\sum_{j} \\alpha_j y_j = 0 $$\n",
    "Substituting these values back in (and simplifying), we obtain:\n",
    "$$ \\max_{\\alpha, \\sum_{j}\\alpha_j y_j =0} \\sum_{j} \\alpha_j - \\frac{1}{2} \\sum_{i,j} y_i y_j \\alpha_i \\alpha_j (x_i.x_j)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO WRITE\n",
    "\n",
    "***\n",
    "<center>___SDCA with Mini-Batches___</center>\n",
    "***\n",
    "___Let___ $w^{(0)} = w(\\alpha^{(0)})$, $\\beta^{(0)} = \\beta_k$, $\\gamma = 0.95$<br>\n",
    "___Iterate:___ for $t=1,2,...,T:$\n",
    "> Randomly pick $A_t \\subseteq [n]$, where $|A_t|=k$<br>\n",
    "Set $A_t^+ = \\{i \\in A_t:y_i \\langle\\, w_{t-1},x_i\\rangle < 1 \\}$ <br>\n",
    "Set $\\eta_t = \\frac{1}{\\lambda t}$<br>\n",
    "Set $w_t \\leftarrow (1-\\eta_t \\lambda)w_{t-1} + \\frac{\\eta_t}{k}\\sum\\limits_{i \\in A_t^+} y_i x_i$<br>\n",
    "[Optional: $w_t \\leftarrow min\\big\\{1, \\frac{1/\\sqrt{\\lambda}}{||w_t||}\\big\\}w_t$]\n",
    "\n",
    " \n",
    "___Output:___<br>\n",
    "> return $w_T$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dual_loss(alpha_, Q_, lambda_, n):\n",
    "    return -alpha_.T.dot(Q_).dot(alpha_) / (2*lambda_*n**2) + np.mean(alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SDCA_mini_batch(X, y, k = 10, lambda_ = 1, gamma = .95, nb_iteration = 100000, output = 'averaging'):\n",
    "    n = X.shape[0]\n",
    "    alpha = [np.zeros(n)]\n",
    "    w = [dual_primal(alpha[0], X, lambda_)]\n",
    "    beta = 1 + ((k-1)*(np.linalg.norm(X.dot(X.T), 2))-1) / (n-1)\n",
    "    Q = X.dot(X.T)\n",
    "    for t in tqdm(range(nb_iteration)):\n",
    "        A = np.random.choice(np.arange(n), size=k, replace=False)\n",
    "        delta_tilde = np.array([np.clip(lambda_*n*(1-y[i]*X[i].dot(w[t])) / beta, -alpha[t][i], 1-alpha[t][i]) for i in A])\n",
    "        zeta = delta_tilde.dot(delta_tilde)\n",
    "        Delta_tilde = np.sum([delta_tilde[i]*y[j]*X[j] for i, j in enumerate(A)], axis = 1)\n",
    "        rho = np.clip(np.linalg.norm(Delta_tilde, 2) / zeta, 1, beta)\n",
    "        delta_A = np.array([np.clip(lambda_*n*(1-y[i]*X[i].dot(w[t])) / rho, -alpha[t][i], 1-alpha[t][i]) for i in A])\n",
    "        beta = beta**gamma * rho**(1-gamma)\n",
    "        delta = np.zeros(n)\n",
    "        delta[A] += delta_A\n",
    "        if dual_loss(alpha[t]+delta, Q, lambda_, n) > dual_loss(alpha[t], Q, lambda_, n):\n",
    "            alpha.append(alpha[t]+delta)\n",
    "            w.append(w[t] + (1 / (lambda_*n)) * (delta*y).dot(X))\n",
    "        else:\n",
    "            alpha.append(alpha[t])\n",
    "            w.append(w[t])\n",
    "    if output == 'averaging':\n",
    "        return np.mean(w[int(nb_iteration/2):], axis = 0)\n",
    "    else:\n",
    "        return w[np.random.randint(int(nb_iteration/2), nb_iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/Users/Linger/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "100%|██████████| 10000/10000 [00:17<00:00, 572.67it/s]\n"
     ]
    }
   ],
   "source": [
    "sdca_mini_batch_coefs = SDCA_mini_batch(X, y, k = 1, lambda_ = 1, gamma = .95, nb_iteration = 10000, output = 'averaging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030472054320648524"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(sdca_mini_batch_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___SDCA-Perm___</center>\n",
    "***\n",
    "___Let___ $w^{(0)} = w(\\alpha^{(0)})$<br>\n",
    "___Let___ $t=0$<br>\n",
    "___Iterate:___ for epoch $k=1,2,...$<br>\n",
    "> ___Let___ ${i_1,...,i_n}$ be a random permutation of ${1,...,n}$<br>\n",
    "___Iterate:___ for $j=1,2,...,n:$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $t \\leftarrow t+1$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $i=i_j$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Find $\\Delta\\alpha_i$ to increase dual<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\alpha^{(t)} \\leftarrow \\alpha^{(t-1)}+\\Delta\\alpha_ie_i$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $w^{(t)} \\leftarrow w^{(t-1)}+(\\lambda n)^{-1}\\Delta\\alpha_ix_i$<br>\n",
    " \n",
    "___Output (Averaging option):___<br>\n",
    "> Let $\\bar{\\alpha} = \\frac{1}{T-T_0}\\sum\\limits_{i=T_0+1}^T \\alpha^{(t-1)}$<br>\n",
    "Let $\\bar{w} = w(\\bar{\\alpha}) = \\frac{1}{T-T_0}\\sum\\limits_{i=T_0+1}^T w^{(t-1)}$<br>\n",
    "return $\\bar{w}$<br>\n",
    "\n",
    "___Output (Random option):___<br>\n",
    "> Let $\\bar{\\alpha} = \\alpha^{(t)}$ and $\\bar{w} = w^{(t)}$ for some random $t \\in T_0+1,...T$<br>\n",
    "return $\\bar{w}$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SDCA_perm(X, y, lambda_ = 1, nb_epoch = 100, output = 'averaging'):\n",
    "    n = X.shape[0]\n",
    "    alpha = [np.zeros(n)]\n",
    "    w = [dual_primal(alpha[0], X, lambda_)]\n",
    "    for k in tqdm(range(nb_epoch)):\n",
    "        X_epo, y_epo = shuffle(X, y)\n",
    "        for i in range(n):\n",
    "            A = (1/(lambda_*n))*np.linalg.norm(X_epo[i], 2)**2\n",
    "            B = X_epo[i].dot(w[k*n+i])\n",
    "            delta_alpha = (y_epo[i]-B)/A\n",
    "            delta_alpha_constrained = (1/y_epo[i])*max(0, min(1, y_epo[i]*(delta_alpha+alpha[k*n+i][i]))) - alpha[k*n+i][i]\n",
    "            e_i = np.zeros(n)\n",
    "            e_i[i] = 1.\n",
    "            alpha.append(alpha[k*n+i]+delta_alpha_constrained*e_i)\n",
    "            w.append(dual_primal(alpha[k*n+i+1], X_epo, lambda_))\n",
    "    if output == 'averaging':\n",
    "        return np.mean(w[int(nb_epoch*n/2):], axis = 0)\n",
    "    else:\n",
    "        return w[np.random.randint(int(nb_epoch*n/2), nb_epoch*n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.89it/s]\n"
     ]
    }
   ],
   "source": [
    "SDCA_perm_coefs = SDCA_perm(X, y, 1, 100, output = 'averaging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0079336255270822213"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(SDCA_perm_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3] Stochastic gradient descent for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pegasos algorithm performs stochastic gradient descent on the primal objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___Basic Pegasos algorithm___</center>\n",
    "***\n",
    "___Let___ $w^{(0)} = 0$<br>\n",
    "___Iterate:___ for $t=1,2,...,T:$\n",
    "> Randomly pick $i$<br>\n",
    "Set $\\eta_t = \\frac{1}{\\lambda t}$<br>\n",
    "If $y_i \\langle\\, w_{t-1},x_i\\rangle < 1$:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $w_t \\leftarrow (1-\\eta_t \\lambda)w_{t-1} + \\eta_t y_i x_i$<br>\n",
    "Else:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $w_t \\leftarrow (1-\\eta_t \\lambda)w_{t-1}$<br>\n",
    "[Optional: $w_t \\leftarrow min\\big\\{1, \\frac{1/\\sqrt{\\lambda}}{||w_t||}\\big\\}w_t$]\n",
    "\n",
    " \n",
    "___Output:___<br>\n",
    "> return $w_T$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this algorithm, we need to compute the subgradient of the penalized hinge loss:\n",
    "\\begin{equation}\n",
    "f(w; i_t) = \\frac{\\lambda}{2}||w||^2 + max(0, 1-y_{i_t} \\langle\\,w, x_{i_t}\\rangle)\n",
    "\\end{equation}\n",
    "\n",
    "It is givent by:\n",
    "\\begin{equation}\n",
    "\\bigtriangledown_t =\n",
    "    \\begin{cases}\n",
    "      \\lambda w_t - y_{i_t} x_{i_t} & \\text{if } y_{i_t} \\langle\\,w, x_{i_t}\\rangle < 1 \\\\\n",
    "      \\lambda w_t  & \\text{otherwise}\n",
    "    \\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "We can add the optional projection step to the previous algorithm to limit the set of admissible solutions to the ball of radius $\\frac{1}{\\sqrt{\\lambda}}$. However, in the experiments, there is no major differences between the projected and unprojected variants of Pegasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Pegasos(X, y, lambda_ = 1, nb_iteration = 100000, projection = False):\n",
    "    n = X.shape[0]\n",
    "    w = [np.repeat(0, X.shape[1])]\n",
    "    for t in tqdm(range(1,nb_iteration+1)):\n",
    "        i = np.random.randint(0, n)\n",
    "        eta = 1/(lambda_ * t)\n",
    "        if y[i]*w[t-1].dot(X[i]) < 1:\n",
    "            w_inter = (1-eta*lambda_)*w[t-1] + eta * y[i] * X[i]\n",
    "        else:\n",
    "            w_inter = (1-eta*lambda_)*w[t-1]\n",
    "        if projection:\n",
    "            w.append(min(1, (1/np.sqrt(lambda_))/np.linalg.norm(w_inter, 2))*w_inter)\n",
    "        else:\n",
    "            w.append(w_inter)\n",
    "    return w[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 59612.76it/s]\n"
     ]
    }
   ],
   "source": [
    "Pegasos_coefs = Pegasos(X, y, 1, 100000, projection = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0021100335852331669"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(Pegasos_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mini-batch setting of this algorithm can be used by using k examples at each iteration, where 1 ≤ k ≤ n is a parameter that needs to be provided to the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___Pegasos algorithm with Mini-Batches___</center>\n",
    "***\n",
    "___Let___ $w^{(0)} = 0$<br>\n",
    "___Iterate:___ for $t=1,2,...,T:$\n",
    "> Randomly pick $A_t \\subseteq [n]$, where $|A_t|=k$<br>\n",
    "Set $A_t^+ = \\{i \\in A_t:y_i \\langle\\, w_{t-1},x_i\\rangle < 1 \\}$ <br>\n",
    "Set $\\eta_t = \\frac{1}{\\lambda t}$<br>\n",
    "Set $w_t \\leftarrow (1-\\eta_t \\lambda)w_{t-1} + \\frac{\\eta_t}{k}\\sum\\limits_{i \\in A_t^+} y_i x_i$<br>\n",
    "[Optional: $w_t \\leftarrow min\\big\\{1, \\frac{1/\\sqrt{\\lambda}}{||w_t||}\\big\\}w_t$]\n",
    "\n",
    " \n",
    "___Output:___<br>\n",
    "> return $w_T$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above description we refer to $A_t$ as chosen uniformly at random among the subsets of $[n]$ of size $k$, i.e. chosen without repetitions. Notice that the analysis still holds when $A_t$ is a multi-set chosen i.i.d. with repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Pegasos_mini_batch(X, y, k, lambda_ = 1, nb_iteration = 100000, projection = False):\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    w = [np.repeat(0, d)]\n",
    "    for t in tqdm(range(1,nb_iteration+1)):\n",
    "        A = np.random.choice(np.arange(n), size=k, replace=False)\n",
    "        A_plus = [i for i in A if (y[i]*w[t-1].dot(X[i]) < 1)]\n",
    "        eta = 1/(lambda_ * t)\n",
    "        w_inter = (1-eta*lambda_)*w[t-1] + (eta/k) * y[A_plus].T.dot(X[A_plus])\n",
    "        if projection:\n",
    "            w.append(min(1, (1/np.sqrt(lambda_))/np.linalg.norm(w_inter, 2))*w_inter)\n",
    "        else:\n",
    "            w.append(w_inter)\n",
    "    return w[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 5381.22it/s]\n"
     ]
    }
   ],
   "source": [
    "Pegasos_mini_batch_coefs = Pegasos_mini_batch(X, y, 10, 1, 10000, projection = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019735302810045042"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(Pegasos_mini_batch_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___Pegasos-Perm algorithm___</center>\n",
    "***\n",
    "___Let___ $w^{(0)} = 0$<br>\n",
    "___Let___ $t=0$<br>\n",
    "___Iterate:___ for epoch $k=1,2,...$<br>\n",
    "> ___Let___ ${i_1,...,i_n}$ be a random permutation of ${1,...,n}$<br>\n",
    "___Iterate:___ for $j=1,2,...,n:$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $t \\leftarrow t+1$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $i=i_j$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Set $\\eta_t = \\frac{1}{\\lambda t}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;If $y_i \\langle\\, w_{t-1},x_i\\rangle < 1$:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $w_t \\leftarrow (1-\\eta_t \\lambda)w_{t-1} + \\eta_t y_i x_i$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Else:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $w_t \\leftarrow (1-\\eta_t \\lambda)w_{t-1}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; [Optional: $w_t \\leftarrow min\\big\\{1, \\frac{1/\\sqrt{\\lambda}}{||w_t||}\\big\\}w_t$]\n",
    " \n",
    "___Output:___<br>\n",
    "> return $w_T$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Pegasos_perm(X, y, lambda_ = 1, nb_epoch = 100, projection = False):\n",
    "    n = X.shape[0]\n",
    "    w = [np.repeat(0, X.shape[1])]\n",
    "    for k in tqdm(range(nb_epoch)):\n",
    "        X_epo, y_epo = shuffle(X, y)\n",
    "        for i in range(n):\n",
    "            eta = 1/(lambda_ * (n*k+i+1))\n",
    "            if y[i]*w[n*k+i].dot(X[i]) < 1:\n",
    "                w_inter = (1-eta*lambda_)*w[n*k+i] + eta * y[i] * X[i]\n",
    "            else:\n",
    "                w_inter = (1-eta*lambda_)*w[n*k+i]\n",
    "            if projection:\n",
    "                w.append(min(1, (1/np.sqrt(lambda_))/np.linalg.norm(w_inter, 2))*w_inter)\n",
    "            else:\n",
    "                w.append(w_inter)\n",
    "    return w[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 94.48it/s]\n"
     ]
    }
   ],
   "source": [
    "Pegasos_perm_coefs = Pegasos_perm(X, y, lambda_ = 1, nb_epoch = 100, projection = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0043903122515433777"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(Pegasos_perm_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4] Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7000, 15), (3000, 15))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', header=None)\n",
    "data = data.dropna().sample(10000)\n",
    "X = data.drop([15], axis = 1).as_matrix()\n",
    "y = data[15].as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.14783828   0.17499591   0.26872916  -0.02998399  13.11964698\n",
      "  -12.67739679  -1.66141667  -8.40954428  -2.34395459   2.03604741\n",
      "    2.85744789   0.02489517   7.13596509   0.26150137  -1.55759558]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "566.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.coef_)\n",
    "pred_sklearn = clf.predict(X_test)\n",
    "sum(abs(pred_sklearn - y_test)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [00:01<00:00, 6215.43it/s]\n"
     ]
    }
   ],
   "source": [
    "n = X_train.shape[0]\n",
    "mini_batch_size = 100\n",
    "for nb_epoch in [1]:\n",
    "    sdca_coefs = SDCA(X_train, y_train, 1, nb_epoch*n, output = 'averaging')\n",
    "    sdca_mini_batch_coefs = SDCA_mini_batch(X_train, y_train, mini_batch_size, 1, 0.95, int(nb_epoch*n/mini_batch_size), output = 'averaging')\n",
    "    sdca_perm_coefs = SDCA_perm(X_train, y_train, nb_epoch, output = 'averaging')\n",
    "    Pegasos_coefs = Pegasos(X_train, y_train, 1, nb_epoch*n, projection = False)\n",
    "    Pegasos_mini_batch_coefs = Pegasos_mini_batch(X_train, y_train, 1, int(nb_epoch*n/mini_batch_size), projection = False)\n",
    "    Pegasos_perm_coefs = Pegasos_perm(X_train, y_train, 1, nb_epoch, projection = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_sdca = sdca_coefs.dot(X_test.T)\n",
    "pred_sdca_mini_batch = sdca_mini_batch_coefs.dot(X_test.T)\n",
    "pred_sdca_perm = sdca_perm_coefs.dot(X_test.T)\n",
    "pred_Pegasos = Pegasos_coefs.dot(X_test.T)\n",
    "pred_Pegasos_mini_batch = Pegasos_mini_batch_coefs.dot(X_test.T)\n",
    "pred_Pegasos_perm = Pegasos_perm_coefs.dot(X_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210000/210000 [01:00<00:00, 3475.49it/s]\n"
     ]
    }
   ],
   "source": [
    "n = X_train.shape[0]\n",
    "mini_batch_size = 100\n",
    "nb_epoch = 30\n",
    "sdca_coefs = SDCA(X_train, y_train, .0000001, nb_epoch*n, output = 'averaging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.16486104,  0.23675861, -0.09395069, -0.47532035,  8.88647787,\n",
       "       -8.49966549, -1.79499911, -6.06723617, -1.97788422,  1.57210103,\n",
       "        1.18664016,  0.18844858,  5.56409705,  0.29250989, -0.38879925])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdca_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627.0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sdca = sdca_coefs.dot(X_test.T)\n",
    "pred_sdca = (pred_sdca > 0).astype(int)\n",
    "np.place(pred_sdca, pred_sdca==0, -1)\n",
    "sum(abs(pred_sdca - y_test)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___The kernalized Pegasos algorithm___</center>\n",
    "***\n",
    "___Let___ $\\alpha^{(0)} = 0$<br>\n",
    "___Iterate:___ for $t=1,2,...,T:$\n",
    ">  Randomly pick $i$<br>\n",
    "For all $j\\neq i$:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Set $\\alpha_t[j] = \\alpha_{t-1}[j]$<br>\n",
    "If $y_i \\frac{1}{\\lambda t}\\sum\\limits_j \\alpha_{t-1}[j]y_i K(x_i,x_j) < 1:$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Set $\\alpha_t[i] = \\alpha_{t-1}[i]+1$<br>\n",
    "Else:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Set $\\alpha_t[i] = \\alpha_{t-1}[i]$<br>\n",
    " \n",
    "___Output:___<br>\n",
    "> return $\\alpha_T$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernels = {'linear_kernel':kernel.linear_kernel, 'polynomial_kernel':kernel.polynomial_kernel, 'rbf_kernel':kernel.rbf_kernel,\n",
    "          'sigmoid_kernel':kernel.sigmoid_kernel, 'chi2_kernel':kernel.chi2_kernel}\n",
    "def Pegasos_kernalized(X, y, kernel_choice = 'linear_kernel', lambda_ = 1, nb_iteration = 1000, projection = False):\n",
    "    n = X.shape[0]\n",
    "    alpha = [np.repeat(0, n)]\n",
    "    ker_fun = kernels[kernel_choice]\n",
    "    for t in tqdm(range(1,nb_iteration+1)):\n",
    "        i = np.random.randint(0, n)\n",
    "        j = [k for k in np.arange(n) if k!=i]\n",
    "        if y[i]/(lambda_ * t) * sum([alpha[t-1][l] * y[i] * ker_fun(X[i].reshape(1, -1), X[l].reshape(1, -1)).item() for l in j]) < 1:\n",
    "            e_i = np.zeros(n)\n",
    "            e_i[i] = 1\n",
    "            alpha.append(alpha[t-1]+e_i)\n",
    "        else:\n",
    "            alpha.append(alpha[t-1])\n",
    "    return dual_primal(alpha[-1], X, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:02<00:00, 16.10it/s]\n"
     ]
    }
   ],
   "source": [
    "Pegasos_kernalized_coefs = Pegasos_kernalized(X, y, 'linear_kernel', 1, 1000, projection = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12720778759249748"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(Pegasos_kernalized_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernalized SVM solved thanks to kernalized Pegasos algorithm looses its performances compared to the non kernalized version. It seems that the kernalized version of Pegasos is not well suited to solve non-linear SVM if we cannot represent the kernel as a dot product of finite-dimensional feature vectors (i.e. it yields bad results when used with the RBF Kernel for example (which corresponds to infinite-dimensional feature vectors)).\n",
    "\n",
    "See https://www.quora.com/Is-Pegasos-a-good-algorithm-for-non-linear-SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this third part, we merge the 2 algorithms together in order to get better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___Procedure Modified-SGD___</center>\n",
    "***\n",
    "___Initialize:___ $w^{(0)} = 0$<br>\n",
    "___Iterate:___ for $t=1,2,...,n:$<br>\n",
    "> Find $\\alpha_t$ to maximize $-\\phi_t^*(-\\alpha_t) - \\frac{\\lambda t}{2} ||w^{(t-1)}+(\\lambda t)^{-1} \\alpha_tx_t||^2$<br>\n",
    "Let $w^{(t)} = \\frac{1}{\\lambda t}\\sum\\limits_{i=1}^t \\alpha_ix_i$<br>\n",
    "return $\\alpha$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___Procedure SDCA with SGD Initialization___</center>\n",
    "***\n",
    "___Stage 1:___ call Procedure Modified-SGD and obtain $\\alpha$<br>\n",
    "___Stage 2:___ call Procedure SDCA with parameter $\\alpha^{(0)} = \\alpha$<br>\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
