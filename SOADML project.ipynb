{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create a classification dataset and we use the Sklearn linear SVM to see if we get the same weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=20)\n",
    "np.place(y, y==0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X, y)\n",
    "sklearn_coefs = clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider:\n",
    "- $x_1,..., x_n$ $\\in$ $\\mathbb{R}^d$\n",
    "- $y_1,...,y_n$ $\\in$ $(-1, 1)$\n",
    "- $\\lambda > 0$ a regularization parameter\n",
    "- $\\phi_1,...,\\phi_n$ a sequence of scalars convex functions\n",
    "- Since we are working on the SVM problem (with linear kernels and no bias term), we set $\\phi_i(a) = max(0,1-y_ia)$ (Hinge loss)\n",
    "\n",
    "For solving SVM, there are 2 approaches:\n",
    "- stochastic gradient descent (SGD), which aims to minimize the following primal problem:\n",
    "\\begin{equation}\n",
    "min_{w \\text{ } \\in \\text{ } R^d} \\big[ \\frac{1}{n} \\sum\\limits_{i=1}^n \\phi_i(w^Tx_i) + \\frac{\\lambda}{2}||w||^2\\big]\n",
    "\\end{equation}\n",
    "- dual coordinate ascent (DCA), which aims to maximize the following dual problem:\n",
    "\\begin{equation}\n",
    "max_{\\alpha \\text{ } \\in \\text{ } R^n} \\big[ \\frac{1}{n} \\sum\\limits_{i=1}^n -\\phi_i^*(-\\alpha_i) - \\frac{\\lambda}{2} \\big|\\big|\\frac{1}{\\lambda n}\\sum\\limits_{i=1}^n \\alpha_i x_i \\big|\\big|^2\\big]\n",
    "\\end{equation}\n",
    "here, for each $i$, $\\phi_i^*: \\mathbb{R} \\rightarrow \\mathbb{R}$ is the convex conjugate of $\\phi_i$, namely $\\phi_i^*(u) = max_z(zu - \\phi_i(z))$\n",
    "\n",
    "If we define, $w(\\alpha) = \\frac{1}{\\lambda n}\\sum\\limits_{i=1}^n \\alpha_i x_i$, then we have that $w(\\alpha^*) = w^*$ where $\\alpha^*$ is an optimal solution of the dual problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___Procedure SDCA($\\alpha^{(0)}$)___</center>\n",
    "***\n",
    "___Let___ $w^{(0)} = w(\\alpha^{(0)})$<br>\n",
    "___Iterate:___ for $t=1,2,...,T:$\n",
    "> Randomly pick $i$<br>\n",
    "Find $\\Delta\\alpha_i$ to maximize $-\\phi_i^*(-(\\alpha_i^{(t-1)}+\\Delta\\alpha_i)) - \\frac{\\lambda n}{2} ||w^{(t-1)}+(\\lambda n)^{-1} \\Delta\\alpha_ix_i||^2$<br>\n",
    "$\\alpha^{(t)} \\leftarrow \\alpha^{(t-1)}+\\Delta\\alpha_ie_i$<br>\n",
    "$w^{(t)} \\leftarrow w^{(t-1)}+(\\lambda n)^{-1}\\Delta\\alpha_ix_i$<br>\n",
    " \n",
    "___Output (Averaging option):___<br>\n",
    "> Let $\\bar{\\alpha} = \\frac{1}{T-T_0}\\sum\\limits_{i=T_0+1}^T \\alpha^{(t-1)}$<br>\n",
    "Let $\\bar{w} = w(\\bar{\\alpha}) = \\frac{1}{T-T_0}\\sum\\limits_{i=T_0+1}^T w^{(t-1)}$<br>\n",
    "return $\\bar{w}$<br>\n",
    "\n",
    "___Output (Random option):___<br>\n",
    "> Let $\\bar{\\alpha} = \\alpha^{(t)}$ and $\\bar{w} = w^{(t)}$ for some random $t \\in T_0+1,...T$<br>\n",
    "return $\\bar{w}$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Close formula to maximize $-\\phi_i^*(-(\\alpha_i^{(t-1)}+\\Delta\\alpha_i)) - \\frac{\\lambda n}{2} ||w^{(t-1)}+(\\lambda n)^{-1} \\Delta\\alpha_ix_i||^2$:___\n",
    "\n",
    "We want to $max_{\\alpha \\text{ } \\in \\text{ } R^n} D(\\alpha)$ with $D(\\alpha) = \\frac{1}{n} \\sum\\limits_{i=1}^n -\\phi_i^*(-\\alpha_i) - \\frac{\\lambda}{2} \\big|\\big|\\frac{1}{\\lambda n}\\sum\\limits_{i=1}^n \\alpha_i x_i \\big|\\big|^2 = - \\frac{1}{2\\lambda n^2} \\alpha^T X^T X \\alpha + \\frac{1}{n} \\sum\\limits_{i=1}^n -\\phi_i^*(-\\alpha_i)$\n",
    "\n",
    "At each iteration, we have to maximize the updated dual objective defined as:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "D(\\alpha_t+\\Delta\\alpha_i e_i) = &\\frac{-1}{2\\lambda n^2} (\\alpha_t+\\Delta\\alpha_i e_i)^T X^T X (\\alpha_t+\\Delta\\alpha_i e_i) - \\frac{1}{n}\\sum\\limits_{i=1}^n \\phi_i^*(-\\alpha_t-\\Delta\\alpha_i e_i)\\\\\n",
    "=& \\frac{-1}{2\\lambda n^2} \\alpha_t^T X^T X \\alpha_t - \\frac{1}{\\lambda n^2} \\alpha_t^T X^T X \\Delta\\alpha_ie_i - \\frac{1}{2\\lambda n^2} (\\Delta\\alpha_i e_i)^T X^T X (\\Delta\\alpha_i e_i) - \\frac{1}{n}\\sum\\limits_{i=1}^n \\phi_i^*(-\\alpha_t-\\Delta\\alpha_i e_i)\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "By setting:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "Constant =& \\frac{-1}{2\\lambda n^2} \\alpha_t^T X^T X \\alpha_t - \\frac{1}{n}\\sum\\limits_{\\substack{i=1 \\\\ i\\neq j}}^n \\phi_j^*(-\\alpha_t-\\Delta\\alpha_j e_j)\\\\\n",
    "A =& \\frac{1}{\\lambda n} x_i^T x_i = \\frac{1}{\\lambda n}||x_i||^2\\\\\n",
    "B=& x_i^T \\frac{X \\alpha_t}{\\lambda n} = x_i^T w_t\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "we get:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "D(\\alpha_t+\\Delta\\alpha_i e_i) \\propto \\frac{-A}{2} (\\Delta\\alpha_i)^2 - B \\Delta\\alpha_i - \\phi^*_i(-\\alpha_i\\Delta\\alpha_i)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We recall that in the case of SVM, we use the Hinge loss, i.e.:\n",
    "\\begin{equation}\n",
    "\\phi_i(a) = max(0,1-y_ia)\n",
    "\\end{equation}\n",
    "and its conjugate is given by:\n",
    "\\begin{equation}\n",
    "\\phi^*_i(u) =\n",
    "    \\begin{cases}\n",
    "      y_i u & \\text{if} -1\\leq y_iu\\leq 0\\\\\n",
    "      +\\infty & \\text{otherwise}\n",
    "    \\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "Then, considering that $-1\\leq y_iu\\leq 0$, we can maximize $D(\\alpha_t+\\Delta\\alpha_i e_i)$ by $\\Delta\\alpha_i$ by:\n",
    "\\begin{equation}\n",
    "\\tilde{\\Delta\\alpha_i} = \\frac{y_i -B}{A}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, if we incorporate the constraint $-1\\leq -y_i(\\alpha_i + \\Delta\\alpha_i)\\leq 0 \\leftrightarrow 0\\leq y_i(\\alpha_i + \\Delta\\alpha_i)\\leq 1$, we obtain the update:\n",
    "\\begin{equation}\n",
    "\\Delta\\alpha_i = \\frac{1}{y_i}max(0, min(1, y_i(\\alpha_i + \\tilde{\\Delta\\alpha_i}))) - \\alpha_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dual_primal(alpha, X, lambda_):\n",
    "    return X.T.dot(alpha)/(lambda_*X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def SDCA(nb_iteration, X, y, lambda_ = 1, output = 'averaging'):\n",
    "    n = X.shape[0]\n",
    "    alpha = [np.zeros(n)]\n",
    "    w = [dual_primal(alpha[0], X, lambda_)]\n",
    "    for t in tqdm(range(nb_iteration)):\n",
    "        i = np.random.randint(0, n)\n",
    "        A = (1/(lambda_*n))*np.linalg.norm(X[i], 2)**2\n",
    "        B = X[i].dot(w[t])\n",
    "        delta_alpha = (y[i]-B)/A\n",
    "        delta_alpha_constrained = (1/y[i])*max(0, min(1, y[i]*(delta_alpha+alpha[t][i]))) - alpha[t][i]\n",
    "        e_i = np.zeros(n)\n",
    "        e_i[i] = 1.\n",
    "        alpha.append(alpha[t]+delta_alpha_constrained*e_i)\n",
    "        w.append(dual_primal(alpha[t+1], X, lambda_))\n",
    "    if output == 'averaging':\n",
    "        return np.mean(w[int(nb_iteration/2):], axis = 0)\n",
    "    else:\n",
    "        return w[np.random.randint(int(nb_iteration/2), nb_iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:07<00:00, 13091.53it/s]\n"
     ]
    }
   ],
   "source": [
    "sdca_coefs = SDCA(100000, X, y, 1, output = 'averaging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0081328491042149797"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(sdca_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___Procedure Modified-SGD___</center>\n",
    "***\n",
    "___Initialize:___ $w^{(0)} = 0$<br>\n",
    "___Iterate:___ for $t=1,2,...,n:$<br>\n",
    "> Find $\\alpha_t$ to maximize $-\\phi_t^*(-\\alpha_t) - \\frac{\\lambda t}{2} ||w^{(t-1)}+(\\lambda t)^{-1} \\alpha_tx_t||^2$<br>\n",
    "Let $w^{(t)} = \\frac{1}{\\lambda t}\\sum\\limits_{i=1}^t \\alpha_ix_i$<br>\n",
    "return $\\alpha$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___Procedure SDCA with SGD Initialization___</center>\n",
    "***\n",
    "___Stage 1:___ call Procedure Modified-SGD and obtain $\\alpha$<br>\n",
    "___Stage 2:___ call Procedure SDCA with parameter $\\alpha^{(0)} = \\alpha$<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___Procedure SDCA-Perm($\\alpha^{(0)}$)___</center>\n",
    "***\n",
    "___Let___ $w^{(0)} = w(\\alpha^{(0)})$<br>\n",
    "___Let___ $t=0$<br>\n",
    "___Iterate:___ for epoch $k=1,2,...$<br>\n",
    "> ___Let___ ${i_1,...,i_n}$ be a random permutation of ${1,...,n}$<br>\n",
    "___Iterate:___ for $j=1,2,...,n:$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $t \\leftarrow t+1$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $i=i_j$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Find $\\Delta\\alpha_i$ to increase dual<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\alpha^{(t)} \\leftarrow \\alpha^{(t-1)}+\\Delta\\alpha_ie_i$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $w^{(t)} \\leftarrow w^{(t-1)}+(\\lambda n)^{-1}\\Delta\\alpha_ix_i$<br>\n",
    " \n",
    "___Output (Averaging option):___<br>\n",
    "> Let $\\bar{\\alpha} = \\frac{1}{T-T_0}\\sum\\limits_{i=T_0+1}^T \\alpha^{(t-1)}$<br>\n",
    "Let $\\bar{w} = w(\\bar{\\alpha}) = \\frac{1}{T-T_0}\\sum\\limits_{i=T_0+1}^T w^{(t-1)}$<br>\n",
    "return $\\bar{w}$<br>\n",
    "\n",
    "___Output (Random option):___<br>\n",
    "> Let $\\bar{\\alpha} = \\alpha^{(t)}$ and $\\bar{w} = w^{(t)}$ for some random $t \\in T_0+1,...T$<br>\n",
    "return $\\bar{w}$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def SDCA_perm(nb_epoch, X, y, lambda_ = 1, output = 'averaging'):\n",
    "    n = X.shape[0]\n",
    "    alpha = [np.zeros(n)]\n",
    "    w = [dual_primal(alpha[0], X, lambda_)]\n",
    "    for k in tqdm(range(nb_epoch)):\n",
    "        X_epo, y_epo = shuffle(X, y)\n",
    "        for i in range(n):\n",
    "            A = (1/(lambda_*n))*np.linalg.norm(X_epo[i], 2)**2\n",
    "            B = X_epo[i].dot(w[k*n+i])\n",
    "            delta_alpha = (y_epo[i]-B)/A\n",
    "            delta_alpha_constrained = (1/y_epo[i])*max(0, min(1, y_epo[i]*(delta_alpha+alpha[k*n+i][i]))) - alpha[k*n+i][i]\n",
    "            e_i = np.zeros(n)\n",
    "            e_i[i] = 1.\n",
    "            alpha.append(alpha[k*n+i]+delta_alpha_constrained*e_i)\n",
    "            w.append(dual_primal(alpha[k*n+i+1], X_epo, lambda_))\n",
    "    if output == 'averaging':\n",
    "        return np.mean(w[int(nb_epoch*n/2):], axis = 0)\n",
    "    else:\n",
    "        return w[np.random.randint(int(nb_epoch*n/2), nb_epoch*n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 15.21it/s]\n"
     ]
    }
   ],
   "source": [
    "SDCA_perm_coefs = SDCA_perm(100, X, y, lambda_ = 1, output = 'averaging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012413574629476157"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(SDCA_perm_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pegasos algorithm performs stochastic gradient descent on the primal objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___The basic Pegasos algorithm___</center>\n",
    "***\n",
    "___Let___ $w^{(0)} = 0$<br>\n",
    "___Iterate:___ for $t=1,2,...,T:$\n",
    "> Randomly pick $i$<br>\n",
    "Set $\\eta_t = \\frac{1}{\\lambda t}$<br>\n",
    "If $y_i \\langle\\, w_{t-1},x_i\\rangle < 1$:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $w_t \\leftarrow (1-\\eta_t \\lambda)w_{t-1} + \\eta_t y_i x_i$<br>\n",
    "Else:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $w_t \\leftarrow (1-\\eta_t \\lambda)w_{t-1}$<br>\n",
    "[Optional: $w_t \\leftarrow min\\big\\{1, \\frac{1/\\sqrt{\\lambda}}{||w_t||}\\big\\}w_t$]\n",
    "\n",
    " \n",
    "___Output:___<br>\n",
    "> return $w_T$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this algorithm, we need to compute the subgradient of the penalized hinge loss:\n",
    "\\begin{equation}\n",
    "f(w; i_t) = \\frac{\\lambda}{2}||w||^2 + max(0, 1-y_{i_t} \\langle\\,w, x_{i_t}\\rangle)\n",
    "\\end{equation}\n",
    "\n",
    "It is givent by:\n",
    "\\begin{equation}\n",
    "\\bigtriangledown_t =\n",
    "    \\begin{cases}\n",
    "      \\lambda w_t - y_{i_t} x_{i_t} & \\text{if } y_{i_t} \\langle\\,w, x_{i_t}\\rangle < 1 \\\\\n",
    "      \\lambda w_t  & \\text{otherwise}\n",
    "    \\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "We can add the optional projection step to the previous algorithm to limit the set of admissible solutions to the ball of radius $\\frac{1}{\\sqrt{\\lambda}}$. However, in the experiments, there is no major differences between the projected and unprojected variants of Pegasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Pegasos(nb_iteration, X, y, lambda_ = 1, projection = False):\n",
    "    n = X.shape[0]\n",
    "    w = [np.repeat(0, X.shape[1])]\n",
    "    for t in tqdm(range(1,nb_iteration+1)):\n",
    "        i = np.random.randint(0, n)\n",
    "        eta = 1/(lambda_ * t)\n",
    "        if y[i]*w[t-1].dot(X[i]) < 1:\n",
    "            w_inter = (1-eta*lambda_)*w[t-1] + eta * y[i] * X[i]\n",
    "        else:\n",
    "            w_inter = (1-eta*lambda_)*w[t-1]\n",
    "        if projection:\n",
    "            w.append(min(1, (1/np.sqrt(lambda_))/np.linalg.norm(w_inter, 2))*w_inter)\n",
    "        else:\n",
    "            w.append(w_inter)\n",
    "    return w[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 67725.76it/s]\n"
     ]
    }
   ],
   "source": [
    "Pegasos_coefs = Pegasos(100000, X, y, lambda_ = 1, projection = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0084390124220156144"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(Pegasos_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mini-batch setting of this algorithm can be used by using k examples at each iteration, where 1 ≤ k ≤ n is a parameter that needs to be provided to the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___The mini-batch Pegasos algorithm___</center>\n",
    "***\n",
    "___Let___ $w^{(0)} = 0$<br>\n",
    "___Iterate:___ for $t=1,2,...,T:$\n",
    "> Randomly pick $A_t \\subseteq [n]$, where $|A_t|=k$<br>\n",
    "Set $A_t^+ = \\{i \\in A_t:y_i \\langle\\, w_{t-1},x_i\\rangle < 1 \\}$ <br>\n",
    "Set $\\eta_t = \\frac{1}{\\lambda t}$<br>\n",
    "Set $w_t \\leftarrow (1-\\eta_t \\lambda)w_{t-1} + \\frac{\\eta_t}{k}\\sum\\limits_{i \\in A_t^+} y_i x_i$<br>\n",
    "[Optional: $w_t \\leftarrow min\\big\\{1, \\frac{1/\\sqrt{\\lambda}}{||w_t||}\\big\\}w_t$]\n",
    "\n",
    " \n",
    "___Output:___<br>\n",
    "> return $w_T$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above description we refer to $A_t$ as chosen uniformly at random among the subsets of $[n]$ of size $k$, i.e. chosen without repetitions. Notice that the analysis still holds when $A_t$ is a multi-set chosen i.i.d. with repetitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___The kernalized Pegasos algorithm___</center>\n",
    "***\n",
    "___Let___ $\\alpha^{(0)} = 0$<br>\n",
    "___Iterate:___ for $t=1,2,...,T:$\n",
    ">  Randomly pick $i$<br>\n",
    "For all $j\\neq i$:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Set $\\alpha_t[j] = \\alpha_{t-1}[j]$<br>\n",
    "If $y_i \\frac{1}{\\lambda t}\\sum\\limits_j \\alpha_{t-1}[j]y_i K(x_i,x_j) < 1:$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Set $\\alpha_t[i] = \\alpha_{t-1}[i]+1$<br>\n",
    "Else:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Set $\\alpha_t[i] = \\alpha_{t-1}[i]$<br>\n",
    " \n",
    "___Output:___<br>\n",
    "> return $\\alpha_T$\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
