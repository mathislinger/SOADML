{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create a classification dataset and we use the Sklearn linear SVM to see if we get the same weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=20)\n",
    "np.place(y, y==0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X, y)\n",
    "sklearn_coefs = clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider:\n",
    "- $x_1,..., x_n$ $\\in$ $\\mathbb{R}^d$\n",
    "- $y_1,...,y_n$ $\\in$ $(-1, 1)$\n",
    "- $\\lambda > 0$ a regularization parameter\n",
    "- $\\phi_1,...,\\phi_n$ a sequence of scalars convex functions\n",
    "- Since we are working on the SVM problem (with linear kernels and no bias term), we set $\\phi_i(a) = max(0,1-y_ia)$ (Hinge loss)\n",
    "\n",
    "For solving SVM, there are 2 approaches:\n",
    "- stochastic gradient descent (SGD), which aims to minimize the following primal problem:\n",
    "\\begin{equation}\n",
    "min_{w \\text{ } \\in \\text{ } R^d} \\big[ \\frac{1}{n} \\sum\\limits_{i=1}^n \\phi_i(w^Tx_i) + \\frac{\\lambda}{2}||w||^2\\big]\n",
    "\\end{equation}\n",
    "- dual coordinate ascent (DCA), which aims to maximize the following dual problem:\n",
    "\\begin{equation}\n",
    "max_{\\alpha \\text{ } \\in \\text{ } R^n} \\big[ \\frac{1}{n} \\sum\\limits_{i=1}^n -\\phi_i^*(-\\alpha_i) - \\frac{\\lambda}{2} \\big|\\big|\\frac{1}{\\lambda n}\\sum\\limits_{i=1}^n \\alpha_i x_i \\big|\\big|^2\\big]\n",
    "\\end{equation}\n",
    "here, for each $i$, $\\phi_i^*: \\mathbb{R} \\rightarrow \\mathbb{R}$ is the convex conjugate of $\\phi_i$, namely $\\phi_i^*(u) = max_z(zu - \\phi_i(z))$\n",
    "\n",
    "If we define, $w(\\alpha) = \\frac{1}{\\lambda n}\\sum\\limits_{i=1}^n \\alpha_i x_i$, then we have that $w(\\alpha^*) = w^*$ where $\\alpha^*$ is an optimal solution of the dual problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___Procedure SDCA($\\alpha^{(0)}$)___</center>\n",
    "***\n",
    "___Let___ $w^{(0)} = w(\\alpha^{(0)})$<br>\n",
    "___Iterate:___ for $t=1,2,...,T:$\n",
    "> Randomly pick $i$<br>\n",
    "Find $\\Delta\\alpha_i$ to maximize $-\\phi_i^*(-(\\alpha_i^{(t-1)}+\\Delta\\alpha_i)) - \\frac{\\lambda n}{2} ||w^{(t-1)}+(\\lambda n)^{-1} \\Delta\\alpha_ix_i||^2$<br>\n",
    "$\\alpha^{(t)} \\leftarrow \\alpha^{(t-1)}+\\Delta\\alpha_ie_i$<br>\n",
    "$w^{(t)} \\leftarrow w^{(t-1)}+(\\lambda n)^{-1}\\Delta\\alpha_ix_i$<br>\n",
    " \n",
    "___Output (Averaging option):___<br>\n",
    "> Let $\\bar{\\alpha} = \\frac{1}{T-T_0}\\sum\\limits_{i=T_0+1}^T \\alpha^{(t-1)}$<br>\n",
    "Let $\\bar{w} = w(\\bar{\\alpha}) = \\frac{1}{T-T_0}\\sum\\limits_{i=T_0+1}^T w^{(t-1)}$<br>\n",
    "return $\\bar{w}$<br>\n",
    "\n",
    "___Output (Random option):___<br>\n",
    "> Let $\\bar{\\alpha} = \\alpha^{(t)}$ and $\\bar{w} = w^{(t)}$ for some random $t \\in T_0+1,...T$<br>\n",
    "return $\\bar{w}$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Close formula to maximize $-\\phi_i^*(-(\\alpha_i^{(t-1)}+\\Delta\\alpha_i)) - \\frac{\\lambda n}{2} ||w^{(t-1)}+(\\lambda n)^{-1} \\Delta\\alpha_ix_i||^2$:___\n",
    "\n",
    "We want to $max_{\\alpha \\text{ } \\in \\text{ } R^n} D(\\alpha)$ with $D(\\alpha) = \\frac{1}{n} \\sum\\limits_{i=1}^n -\\phi_i^*(-\\alpha_i) - \\frac{\\lambda}{2} \\big|\\big|\\frac{1}{\\lambda n}\\sum\\limits_{i=1}^n \\alpha_i x_i \\big|\\big|^2 = - \\frac{1}{2\\lambda n^2} \\alpha^T X^T X \\alpha + \\frac{1}{n} \\sum\\limits_{i=1}^n -\\phi_i^*(-\\alpha_i)$\n",
    "\n",
    "At each iteration, we have to maximize the updated dual objective defined as:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "D(\\alpha_t+\\Delta\\alpha_i e_i) = &\\frac{-1}{2\\lambda n^2} (\\alpha_t+\\Delta\\alpha_i e_i)^T X^T X (\\alpha_t+\\Delta\\alpha_i e_i) - \\frac{1}{n}\\sum\\limits_{i=1}^n \\phi_i^*(-\\alpha_t-\\Delta\\alpha_i e_i)\\\\\n",
    "=& \\frac{-1}{2\\lambda n^2} \\alpha_t^T X^T X \\alpha_t - \\frac{1}{\\lambda n^2} \\alpha_t^T X^T X \\Delta\\alpha_ie_i - \\frac{1}{2\\lambda n^2} (\\Delta\\alpha_i e_i)^T X^T X (\\Delta\\alpha_i e_i) - \\frac{1}{n}\\sum\\limits_{i=1}^n \\phi_i^*(-\\alpha_t-\\Delta\\alpha_i e_i)\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "By setting:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "Constant =& \\frac{-1}{2\\lambda n^2} \\alpha_t^T X^T X \\alpha_t - \\frac{1}{n}\\sum\\limits_{\\substack{i=1 \\\\ i\\neq j}}^n \\phi_j^*(-\\alpha_t-\\Delta\\alpha_j e_j)\\\\\n",
    "A =& \\frac{1}{\\lambda n} x_i^T x_i = \\frac{1}{\\lambda n}||x_i||^2\\\\\n",
    "B=& x_i^T \\frac{X \\alpha_t}{\\lambda n} = x_i^T w_t\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "we get:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "D(\\alpha_t+\\Delta\\alpha_i e_i) \\propto \\frac{-A}{2} (\\Delta\\alpha_i)^2 - B \\Delta\\alpha_i - \\phi^*_i(-\\alpha_i\\Delta\\alpha_i)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We recall that in the case of SVM, we use the Hinge loss, i.e.:\n",
    "\\begin{equation}\n",
    "\\phi_i(a) = max(0,1-y_ia)\n",
    "\\end{equation}\n",
    "and its conjugate is given by:\n",
    "\\begin{equation}\n",
    "\\phi^*_i(u) =\n",
    "    \\begin{cases}\n",
    "      y_i u & \\text{if} -1\\leq y_iu\\leq 0\\\\\n",
    "      +\\infty & \\text{otherwise}\n",
    "    \\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "Then, considering that $-1\\leq y_iu\\leq 0$, we can maximize $D(\\alpha_t+\\Delta\\alpha_i e_i)$ by $\\Delta\\alpha_i$ by:\n",
    "\\begin{equation}\n",
    "\\tilde{\\Delta\\alpha_i} = \\frac{y_i -B}{A}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, if we incorporate the constraint $-1\\leq -y_i(\\alpha_i + \\Delta\\alpha_i)\\leq 0 \\leftrightarrow 0\\leq y_i(\\alpha_i + \\Delta\\alpha_i)\\leq 1$, we obtain the update:\n",
    "\\begin{equation}\n",
    "\\Delta\\alpha_i = \\frac{1}{y_i}max(0, min(1, y_i(\\alpha_i + \\tilde{\\Delta\\alpha_i}))) - \\alpha_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dual_primal(alpha, X, lambda_):\n",
    "    return X.T.dot(alpha)/(lambda_*X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, we set $T_0 = \\frac{T}{2}$ for the averaging option as suggested in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def SDCA(X, y, lambda_ = 1, nb_iteration = 100000, output = 'averaging'):\n",
    "    n = X.shape[0]\n",
    "    alpha = [np.zeros(n)]\n",
    "    w = [dual_primal(alpha[0], X, lambda_)]\n",
    "    for t in tqdm(range(nb_iteration)):\n",
    "        i = np.random.randint(0, n)\n",
    "        A = (1/(lambda_*n))*np.linalg.norm(X[i], 2)**2\n",
    "        B = X[i].dot(w[t])\n",
    "        delta_alpha = (y[i]-B)/A\n",
    "        delta_alpha_constrained = (1/y[i])*max(0, min(1, y[i]*(delta_alpha+alpha[t][i]))) - alpha[t][i]\n",
    "        e_i = np.zeros(n)\n",
    "        e_i[i] = 1.\n",
    "        alpha.append(alpha[t]+delta_alpha_constrained*e_i)\n",
    "        w.append(dual_primal(alpha[t+1], X, lambda_))\n",
    "    if output == 'averaging':\n",
    "        return np.mean(w[int(nb_iteration/2):], axis = 0)\n",
    "    else:\n",
    "        return w[np.random.randint(int(nb_iteration/2), nb_iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:08<00:00, 12400.97it/s]\n"
     ]
    }
   ],
   "source": [
    "sdca_coefs = SDCA(X, y, 1, 100000, output = 'averaging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00050003589007790897"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(sdca_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found a paper proposing a mini batch version of SDCA. We can implement it in order to compare with the stochastic version. The paper is https://pdfs.semanticscholar.org/b0b5/13b601e28db45a02ed4b19801af0cb29e462.pdf#page3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLAIN HOW TO GET THE FOLLOWING FORMULA\n",
    "\n",
    "To use the following algorithm, we need to compute the loss:\n",
    "http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture6.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dual_loss(alpha_, Q_):\n",
    "    return -alpha_.T.dot(Q_).dot(alpha_) / (2*lambda_*n**2) + np.mean(alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SDCA_mini_batch(X, y, k = 10, lambda_ = 1, gamma = .95, nb_iteration = 100000, output = 'averaging'):\n",
    "    n = X.shape[0]\n",
    "    alpha = [np.zeros(n)]\n",
    "    w = [dual_primal(alpha[0], X, lambda_)]\n",
    "    beta = 1 + ((k-1)*(np.linalg.norm(X.dot(X.T), 2))-1) / (n-1)\n",
    "    Q = X.dot(X.T)\n",
    "    for t in tqdm(range(nb_iteration)):\n",
    "        A = np.random.choice(np.arange(n), size=k, replace=False)\n",
    "        delta_tilde = np.array([np.clip(lambda_*n*(1-y[i]*X[i].dot(w[t])) / beta, -alpha[t][i], 1-alpha[t][i]) for i in A])\n",
    "        zeta = delta_tilde.dot(delta_tilde)\n",
    "        Delta_tilde = np.sum([delta_tilde[i]*y[j]*X[j] for i, j in enumerate(A)], axis = 1)\n",
    "        rho = np.clip(np.linalg.norm(Delta_tilde, 2) / zeta, 1, beta)\n",
    "        delta_A = np.array([np.clip(lambda_*n*(1-y[i]*X[i].dot(w[t])) / rho, -alpha[t][i], 1-alpha[t][i]) for i in A])\n",
    "        beta = beta**gamma * rho**(1-gamma)\n",
    "        delta = np.zeros(n)\n",
    "        delta[A] += delta_A\n",
    "        if dual_loss(alpha[t]+delta, Q) > dual_loss(alpha[t], Q):\n",
    "            alpha.append(alpha[t]+delta)\n",
    "            w.append(w[t] + (1 / (lambda_*n)) * delta*y.dot(X))\n",
    "        else:\n",
    "            alpha.append(alpha[t])\n",
    "            w.append(w[t])\n",
    "    return w[nb_iteration-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 59/10000 [00:00<00:16, 586.26it/s]\u001b[A\n",
      "  1%|          | 103/10000 [00:00<00:18, 528.79it/s]\u001b[A\n",
      "  1%|▏         | 144/10000 [00:00<00:20, 481.42it/s]\u001b[A\n",
      "  2%|▏         | 187/10000 [00:00<00:21, 462.37it/s]\u001b[A\n",
      "  2%|▏         | 230/10000 [00:00<00:21, 446.47it/s]\u001b[A\n",
      "  3%|▎         | 276/10000 [00:00<00:21, 448.55it/s]\u001b[A\n",
      "  3%|▎         | 317/10000 [00:00<00:22, 433.26it/s]\u001b[A\n",
      "  4%|▎         | 360/10000 [00:00<00:22, 428.92it/s]\u001b[A\n",
      "  4%|▍         | 402/10000 [00:00<00:22, 423.31it/s]\u001b[A\n",
      "  4%|▍         | 443/10000 [00:01<00:22, 415.66it/s]\u001b[A\n",
      "  5%|▍         | 487/10000 [00:01<00:22, 420.15it/s]\u001b[A\n",
      "  5%|▌         | 529/10000 [00:01<00:22, 419.18it/s]\u001b[A\n",
      "  6%|▌         | 575/10000 [00:01<00:21, 429.47it/s]\u001b[A\n",
      "  6%|▌         | 621/10000 [00:01<00:21, 435.85it/s]\u001b[A\n",
      "  7%|▋         | 665/10000 [00:01<00:21, 434.10it/s]\u001b[A\n",
      "  7%|▋         | 709/10000 [00:01<00:22, 421.12it/s]\u001b[A\n",
      "  8%|▊         | 753/10000 [00:01<00:21, 423.91it/s]\u001b[A\n",
      "  8%|▊         | 796/10000 [00:01<00:21, 422.69it/s]\u001b[A\n",
      "  8%|▊         | 842/10000 [00:01<00:21, 432.58it/s]\u001b[A\n",
      "  9%|▉         | 886/10000 [00:02<00:21, 425.49it/s]\u001b[A\n",
      "  9%|▉         | 931/10000 [00:02<00:21, 430.68it/s]\u001b[A\n",
      " 10%|▉         | 976/10000 [00:02<00:20, 434.97it/s]\u001b[A\n",
      " 10%|█         | 1027/10000 [00:02<00:19, 453.39it/s]\u001b[A\n",
      " 11%|█         | 1073/10000 [00:02<00:19, 447.68it/s]\u001b[A\n",
      " 11%|█▏        | 1125/10000 [00:02<00:19, 466.07it/s]\u001b[A\n",
      " 12%|█▏        | 1172/10000 [00:02<00:19, 454.85it/s]\u001b[A\n",
      " 12%|█▏        | 1220/10000 [00:02<00:19, 461.52it/s]\u001b[A\n",
      " 13%|█▎        | 1267/10000 [00:02<00:19, 458.19it/s]\u001b[A\n",
      " 13%|█▎        | 1313/10000 [00:02<00:19, 453.95it/s]\u001b[A\n",
      " 14%|█▎        | 1359/10000 [00:03<00:19, 445.67it/s]\u001b[A\n",
      " 14%|█▍        | 1404/10000 [00:03<00:19, 442.18it/s]\u001b[A\n",
      " 14%|█▍        | 1449/10000 [00:03<00:19, 440.92it/s]\u001b[A\n",
      " 15%|█▍        | 1498/10000 [00:03<00:18, 452.32it/s]\u001b[A\n",
      " 15%|█▌        | 1544/10000 [00:03<00:19, 444.10it/s]\u001b[A\n",
      " 16%|█▌        | 1589/10000 [00:03<00:19, 442.19it/s]\u001b[A\n",
      " 16%|█▋        | 1634/10000 [00:03<00:19, 438.42it/s]\u001b[A\n",
      " 17%|█▋        | 1678/10000 [00:03<00:19, 433.87it/s]\u001b[A\n",
      " 17%|█▋        | 1729/10000 [00:03<00:18, 451.08it/s]\u001b[A\n",
      " 18%|█▊        | 1775/10000 [00:04<00:18, 443.53it/s]\u001b[A\n",
      " 18%|█▊        | 1820/10000 [00:04<00:18, 438.62it/s]\u001b[A\n",
      " 19%|█▊        | 1865/10000 [00:04<00:18, 441.44it/s]\u001b[A\n",
      " 19%|█▉        | 1910/10000 [00:04<00:18, 434.34it/s]\u001b[A\n",
      " 20%|█▉        | 1954/10000 [00:04<00:18, 435.92it/s]\u001b[A\n",
      " 20%|█▉        | 1998/10000 [00:04<00:18, 436.51it/s]\u001b[A\n",
      " 20%|██        | 2042/10000 [00:04<00:19, 416.61it/s]\u001b[A\n",
      " 21%|██        | 2088/10000 [00:04<00:18, 426.04it/s]\u001b[A\n",
      " 21%|██▏       | 2131/10000 [00:04<00:18, 425.67it/s]\u001b[A\n",
      " 22%|██▏       | 2174/10000 [00:04<00:18, 421.51it/s]\u001b[A\n",
      " 22%|██▏       | 2217/10000 [00:05<00:18, 412.20it/s]\u001b[A\n",
      " 23%|██▎       | 2259/10000 [00:05<00:19, 401.15it/s]\u001b[A\n",
      " 23%|██▎       | 2303/10000 [00:05<00:18, 410.22it/s]\u001b[A\n",
      " 24%|██▎       | 2350/10000 [00:05<00:18, 424.96it/s]\u001b[A\n",
      " 24%|██▍       | 2393/10000 [00:05<00:17, 424.95it/s]\u001b[A\n",
      " 24%|██▍       | 2437/10000 [00:05<00:17, 427.03it/s]\u001b[A\n",
      " 25%|██▍       | 2480/10000 [00:05<00:17, 421.05it/s]\u001b[A\n",
      " 25%|██▌       | 2523/10000 [00:05<00:18, 402.74it/s]\u001b[A\n",
      " 26%|██▌       | 2564/10000 [00:05<00:18, 400.54it/s]\u001b[A\n",
      " 26%|██▌       | 2607/10000 [00:06<00:18, 407.79it/s]\u001b[A\n",
      " 26%|██▋       | 2650/10000 [00:06<00:17, 413.38it/s]\u001b[A\n",
      " 27%|██▋       | 2694/10000 [00:06<00:17, 418.87it/s]\u001b[A\n",
      " 27%|██▋       | 2740/10000 [00:06<00:16, 429.76it/s]\u001b[A\n",
      " 28%|██▊       | 2784/10000 [00:06<00:16, 427.93it/s]\u001b[A\n",
      " 28%|██▊       | 2829/10000 [00:06<00:16, 432.89it/s]\u001b[A\n",
      " 29%|██▊       | 2873/10000 [00:06<00:16, 432.82it/s]\u001b[A\n",
      " 29%|██▉       | 2917/10000 [00:06<00:16, 422.67it/s]\u001b[A\n",
      "  5%|▌         | 5131/100000 [00:30<09:14, 171.01it/s]\n",
      "100%|██████████| 10000/10000 [00:23<00:00, 429.27it/s]\n"
     ]
    }
   ],
   "source": [
    "sdca_mini_batch_coefs = SDCA_mini_batch(X, y, k = 10, lambda_ = 1, gamma = .95, nb_iteration = 10000, output = 'averaging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056076515775536177"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(sdca_mini_batch_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___Procedure SDCA-Perm($\\alpha^{(0)}$)___</center>\n",
    "***\n",
    "___Let___ $w^{(0)} = w(\\alpha^{(0)})$<br>\n",
    "___Let___ $t=0$<br>\n",
    "___Iterate:___ for epoch $k=1,2,...$<br>\n",
    "> ___Let___ ${i_1,...,i_n}$ be a random permutation of ${1,...,n}$<br>\n",
    "___Iterate:___ for $j=1,2,...,n:$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $t \\leftarrow t+1$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $i=i_j$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Find $\\Delta\\alpha_i$ to increase dual<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\alpha^{(t)} \\leftarrow \\alpha^{(t-1)}+\\Delta\\alpha_ie_i$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $w^{(t)} \\leftarrow w^{(t-1)}+(\\lambda n)^{-1}\\Delta\\alpha_ix_i$<br>\n",
    " \n",
    "___Output (Averaging option):___<br>\n",
    "> Let $\\bar{\\alpha} = \\frac{1}{T-T_0}\\sum\\limits_{i=T_0+1}^T \\alpha^{(t-1)}$<br>\n",
    "Let $\\bar{w} = w(\\bar{\\alpha}) = \\frac{1}{T-T_0}\\sum\\limits_{i=T_0+1}^T w^{(t-1)}$<br>\n",
    "return $\\bar{w}$<br>\n",
    "\n",
    "___Output (Random option):___<br>\n",
    "> Let $\\bar{\\alpha} = \\alpha^{(t)}$ and $\\bar{w} = w^{(t)}$ for some random $t \\in T_0+1,...T$<br>\n",
    "return $\\bar{w}$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def SDCA_perm(X, y, lambda_ = 1, nb_epoch = 100, output = 'averaging'):\n",
    "    n = X.shape[0]\n",
    "    alpha = [np.zeros(n)]\n",
    "    w = [dual_primal(alpha[0], X, lambda_)]\n",
    "    for k in tqdm(range(nb_epoch)):\n",
    "        X_epo, y_epo = shuffle(X, y)\n",
    "        for i in range(n):\n",
    "            A = (1/(lambda_*n))*np.linalg.norm(X_epo[i], 2)**2\n",
    "            B = X_epo[i].dot(w[k*n+i])\n",
    "            delta_alpha = (y_epo[i]-B)/A\n",
    "            delta_alpha_constrained = (1/y_epo[i])*max(0, min(1, y_epo[i]*(delta_alpha+alpha[k*n+i][i]))) - alpha[k*n+i][i]\n",
    "            e_i = np.zeros(n)\n",
    "            e_i[i] = 1.\n",
    "            alpha.append(alpha[k*n+i]+delta_alpha_constrained*e_i)\n",
    "            w.append(dual_primal(alpha[k*n+i+1], X_epo, lambda_))\n",
    "    if output == 'averaging':\n",
    "        return np.mean(w[int(nb_epoch*n/2):], axis = 0)\n",
    "    else:\n",
    "        return w[np.random.randint(int(nb_epoch*n/2), nb_epoch*n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.89it/s]\n"
     ]
    }
   ],
   "source": [
    "SDCA_perm_coefs = SDCA_perm(X, y, 1, 100, output = 'averaging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0079336255270822213"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(SDCA_perm_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pegasos algorithm performs stochastic gradient descent on the primal objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___The basic Pegasos algorithm___</center>\n",
    "***\n",
    "___Let___ $w^{(0)} = 0$<br>\n",
    "___Iterate:___ for $t=1,2,...,T:$\n",
    "> Randomly pick $i$<br>\n",
    "Set $\\eta_t = \\frac{1}{\\lambda t}$<br>\n",
    "If $y_i \\langle\\, w_{t-1},x_i\\rangle < 1$:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $w_t \\leftarrow (1-\\eta_t \\lambda)w_{t-1} + \\eta_t y_i x_i$<br>\n",
    "Else:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $w_t \\leftarrow (1-\\eta_t \\lambda)w_{t-1}$<br>\n",
    "[Optional: $w_t \\leftarrow min\\big\\{1, \\frac{1/\\sqrt{\\lambda}}{||w_t||}\\big\\}w_t$]\n",
    "\n",
    " \n",
    "___Output:___<br>\n",
    "> return $w_T$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this algorithm, we need to compute the subgradient of the penalized hinge loss:\n",
    "\\begin{equation}\n",
    "f(w; i_t) = \\frac{\\lambda}{2}||w||^2 + max(0, 1-y_{i_t} \\langle\\,w, x_{i_t}\\rangle)\n",
    "\\end{equation}\n",
    "\n",
    "It is givent by:\n",
    "\\begin{equation}\n",
    "\\bigtriangledown_t =\n",
    "    \\begin{cases}\n",
    "      \\lambda w_t - y_{i_t} x_{i_t} & \\text{if } y_{i_t} \\langle\\,w, x_{i_t}\\rangle < 1 \\\\\n",
    "      \\lambda w_t  & \\text{otherwise}\n",
    "    \\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "We can add the optional projection step to the previous algorithm to limit the set of admissible solutions to the ball of radius $\\frac{1}{\\sqrt{\\lambda}}$. However, in the experiments, there is no major differences between the projected and unprojected variants of Pegasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Pegasos(X, y, lambda_ = 1, nb_iteration = 100000, projection = False, output = 'averaging'):\n",
    "    n = X.shape[0]\n",
    "    w = [np.repeat(0, X.shape[1])]\n",
    "    for t in tqdm(range(1,nb_iteration+1)):\n",
    "        i = np.random.randint(0, n)\n",
    "        eta = 1/(lambda_ * t)\n",
    "        if y[i]*w[t-1].dot(X[i]) < 1:\n",
    "            w_inter = (1-eta*lambda_)*w[t-1] + eta * y[i] * X[i]\n",
    "        else:\n",
    "            w_inter = (1-eta*lambda_)*w[t-1]\n",
    "        if projection:\n",
    "            w.append(min(1, (1/np.sqrt(lambda_))/np.linalg.norm(w_inter, 2))*w_inter)\n",
    "        else:\n",
    "            w.append(w_inter)\n",
    "    return w[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 59612.76it/s]\n"
     ]
    }
   ],
   "source": [
    "Pegasos_coefs = Pegasos(X, y, 1, 100000, projection = False, output = 'averaging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0021100335852331669"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(Pegasos_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mini-batch setting of this algorithm can be used by using k examples at each iteration, where 1 ≤ k ≤ n is a parameter that needs to be provided to the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___The mini-batch Pegasos algorithm___</center>\n",
    "***\n",
    "___Let___ $w^{(0)} = 0$<br>\n",
    "___Iterate:___ for $t=1,2,...,T:$\n",
    "> Randomly pick $A_t \\subseteq [n]$, where $|A_t|=k$<br>\n",
    "Set $A_t^+ = \\{i \\in A_t:y_i \\langle\\, w_{t-1},x_i\\rangle < 1 \\}$ <br>\n",
    "Set $\\eta_t = \\frac{1}{\\lambda t}$<br>\n",
    "Set $w_t \\leftarrow (1-\\eta_t \\lambda)w_{t-1} + \\frac{\\eta_t}{k}\\sum\\limits_{i \\in A_t^+} y_i x_i$<br>\n",
    "[Optional: $w_t \\leftarrow min\\big\\{1, \\frac{1/\\sqrt{\\lambda}}{||w_t||}\\big\\}w_t$]\n",
    "\n",
    " \n",
    "___Output:___<br>\n",
    "> return $w_T$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above description we refer to $A_t$ as chosen uniformly at random among the subsets of $[n]$ of size $k$, i.e. chosen without repetitions. Notice that the analysis still holds when $A_t$ is a multi-set chosen i.i.d. with repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Pegasos_mini_batch(X, y, k, lambda_ = 1, nb_iteration = 100000, projection = False):\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    w = [np.repeat(0, d)]\n",
    "    for t in tqdm(range(1,nb_iteration+1)):\n",
    "        A = np.random.choice(np.arange(n), size=k, replace=False)\n",
    "        A_plus = [i for i in A if (y[i]*w[t-1].dot(X[i]) < 1)]\n",
    "        eta = 1/(lambda_ * t)\n",
    "        w_inter = (1-eta*lambda_)*w[t-1] + (eta/k) * y[A_plus].T.dot(X[A_plus])\n",
    "        if projection:\n",
    "            w.append(min(1, (1/np.sqrt(lambda_))/np.linalg.norm(w_inter, 2))*w_inter)\n",
    "        else:\n",
    "            w.append(w_inter)\n",
    "    return w[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 5381.22it/s]\n"
     ]
    }
   ],
   "source": [
    "Pegasos_mini_batch_coefs = Pegasos_mini_batch(X, y, 10, 1, 10000, projection = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019735302810045042"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(Pegasos_mini_batch_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___The kernalized Pegasos algorithm___</center>\n",
    "***\n",
    "___Let___ $\\alpha^{(0)} = 0$<br>\n",
    "___Iterate:___ for $t=1,2,...,T:$\n",
    ">  Randomly pick $i$<br>\n",
    "For all $j\\neq i$:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Set $\\alpha_t[j] = \\alpha_{t-1}[j]$<br>\n",
    "If $y_i \\frac{1}{\\lambda t}\\sum\\limits_j \\alpha_{t-1}[j]y_i K(x_i,x_j) < 1:$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Set $\\alpha_t[i] = \\alpha_{t-1}[i]+1$<br>\n",
    "Else:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Set $\\alpha_t[i] = \\alpha_{t-1}[i]$<br>\n",
    " \n",
    "___Output:___<br>\n",
    "> return $\\alpha_T$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics.pairwise as kernel\n",
    "kernels = {'linear_kernel':kernel.linear_kernel, 'polynomial_kernel':kernel.polynomial_kernel, 'rbf_kernel':kernel.rbf_kernel,\n",
    "          'sigmoid_kernel':kernel.sigmoid_kernel, 'chi2_kernel':kernel.chi2_kernel}\n",
    "def Pegasos_kernalized(X, y, kernel_choice = 'linear_kernel', lambda_ = 1, nb_iteration = 1000, projection = False):\n",
    "    n = X.shape[0]\n",
    "    alpha = [np.repeat(0, n)]\n",
    "    ker_fun = kernels[kernel_choice]\n",
    "    for t in tqdm(range(1,nb_iteration+1)):\n",
    "        i = np.random.randint(0, n)\n",
    "        j = [k for k in np.arange(n) if k!=i]\n",
    "        if y[i]/(lambda_ * t) * sum([alpha[t-1][l] * y[i] * ker_fun(X[i].reshape(1, -1), X[l].reshape(1, -1)).item() for l in j]) < 1:\n",
    "            e_i = np.zeros(n)\n",
    "            e_i[i] = 1\n",
    "            alpha.append(alpha[t-1]+e_i)\n",
    "        else:\n",
    "            alpha.append(alpha[t-1])\n",
    "    return dual_primal(alpha[-1], X, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:02<00:00, 16.10it/s]\n"
     ]
    }
   ],
   "source": [
    "Pegasos_kernalized_coefs = Pegasos_kernalized(X, y, 'linear_kernel', 1, 1000, projection = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12720778759249748"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(Pegasos_kernalized_coefs - sklearn_coefs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernalized SVM solved thanks to kernalized Pegasos algorithm looses its performances compared to the non kernalized version. It seems that the kernalized version of Pegasos is not well suited to solve non-linear SVM if we cannot represent the kernel as a dot product of finite-dimensional feature vectors (i.e. it yields bad results when used with the RBF Kernel for example (which corresponds to infinite-dimensional feature vectors)).\n",
    "\n",
    "See https://www.quora.com/Is-Pegasos-a-good-algorithm-for-non-linear-SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this third part, we merge the 2 algorithms together in order to get better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___Procedure Modified-SGD___</center>\n",
    "***\n",
    "___Initialize:___ $w^{(0)} = 0$<br>\n",
    "___Iterate:___ for $t=1,2,...,n:$<br>\n",
    "> Find $\\alpha_t$ to maximize $-\\phi_t^*(-\\alpha_t) - \\frac{\\lambda t}{2} ||w^{(t-1)}+(\\lambda t)^{-1} \\alpha_tx_t||^2$<br>\n",
    "Let $w^{(t)} = \\frac{1}{\\lambda t}\\sum\\limits_{i=1}^t \\alpha_ix_i$<br>\n",
    "return $\\alpha$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>___Procedure SDCA with SGD Initialization___</center>\n",
    "***\n",
    "___Stage 1:___ call Procedure Modified-SGD and obtain $\\alpha$<br>\n",
    "___Stage 2:___ call Procedure SDCA with parameter $\\alpha^{(0)} = \\alpha$<br>\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
